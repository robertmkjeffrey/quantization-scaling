{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a Replication of \"The Quantization Model of Neural Scaling\"\n",
    "https://browse.arxiv.org/pdf/2303.13506.pdf\n",
    "\n",
    "This paper proposes explaining neural scaling as a set of discrete tasks being individually learnt.\n",
    "\n",
    "To demonstrate this, we train a model on a \"Multitask Spare Parity\" dataset. Each example is composed of two sections:\n",
    "\n",
    "$$[\\:\\underbrace{0,1,0,0,}_\\text{Control bits} \\; \\underbrace{1,0,0,1,0,1,1,1,0}_\\text{Data bits}\\:]$$\n",
    "\n",
    "The control bit is a one-hot encoding representing the \"task\" to be computed on the data bits. In this case, the task is Sparse Parity; i.e. the parity of a subset of the data bits.\n",
    "\n",
    "\n",
    "\n",
    "This notebook replicates Figure 7 from the addendum:\n",
    "\n",
    "![Expected training dynamics for multitask parity.](image.png)\n",
    "\n",
    "This plot shows the training curve of each subtask against the loss of the total dataset. We see that each task appears to be learned individually and sharply, while the overall loss has a smooth training curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.generate_data import Sampler, DummyData, MultitaskSparseParity\n",
    "from lib.tracking import AnalyticsManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_bits = 100\n",
    "# n_control_bits = 500\n",
    "n_control_bits = 10\n",
    "k = 3\n",
    "alpha = 0.4\n",
    "\n",
    "sampler: Sampler = MultitaskSparseParity(n_control_bits, n_data_bits, k=k, alpha=alpha)\n",
    "# sampler: Sampler = DummyData(n_control_bits + n_data_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%timeit sampler.generate_data(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20000\n",
    "training_size = 1e5\n",
    "test_size = 1000\n",
    "\n",
    "n_hidden = 200\n",
    "lr = 1e-3\n",
    "n_epochs = 500\n",
    "optimizer_func = lambda model: torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "logger = AnalyticsManager()\n",
    "print_frequency = 30 # Time between logging messages. In between, we use TQDM to show the current loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "    \"\"\"Single hidden layer model with relu activations.\"\"\"\n",
    "    def __init__(self, n_hidden: int):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(n_control_bits + n_data_bits, n_hidden)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(n_hidden, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = TinyModel(n_hidden)\n",
    "optimizer = optimizer_func(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# from time import sleep\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax1 = fig.add_subplot(211)\n",
    "# ax2 = fig.add_subplot(212, sharex=ax1)\n",
    "\n",
    "# for i in range(30):\n",
    "#     ax1.plot(range(i))\n",
    "#     plt.show()\n",
    "#     sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(X, y, model):\n",
    "    y_pred = model(X.float())\n",
    "    loss = loss_func(y_pred, y[:, None].float())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_print_time = 0\n",
    "\n",
    "for epoch in (pbar := tqdm(range(n_epochs))):\n",
    "    for i in range(int(training_size // batch_size)):\n",
    "        X_batch, y_batch = sampler.generate_data(batch_size)\n",
    "\n",
    "        # Calculate total loss\n",
    "        loss = calculate_loss(X_batch, y_batch, model)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        logger.log({\"loss\": loss.item()})\n",
    "\n",
    "        # Calculate loss for each individual task.\n",
    "        for task_index in range(n_control_bits):\n",
    "            X_task, y_task = sampler.generate_data(test_size, force_task=task_index)\n",
    "            with torch.no_grad():\n",
    "                task_loss = calculate_loss(X_task, y_task, model).item()\n",
    "                logger.log({f\"task_{task_index}_loss\" : task_loss})\n",
    "\n",
    "    epoch_str = f\"Epoch: {epoch} loss: {loss.item()}\"\n",
    "    pbar.set_description(epoch_str)\n",
    "    if (time.time() - last_print_time) > print_frequency:\n",
    "        tqdm.write(epoch_str)\n",
    "        last_print_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for key in logger.metrics.keys():\n",
    "    plt.plot(logger.metrics[key], label=key)\n",
    "plt.legend()\n",
    "# plt.yscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
